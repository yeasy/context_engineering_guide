# 14.5 性能优化与部署

系统跑通只是第一步，能在生产环境稳定运行才是关键。

## 1. 性能优化

### 1.1 语义缓存 (Semantic Cache)
对于相似的高频提问，直接返回缓存的答案，无需经过 RAG 流程。
*   使用 `GPTCache` 或 Redis。
*   **原理**：计算 query embedding，若与缓存中问题的相似度 > 0.95，则直接返回。

### 1.2 流式输出 (Streaming)
LLM 生成速度较慢，必须使用 Server-Sent Events (SSE) 或 WebSocket 实现流式响应，让用户立刻看到首字，降低心理延迟。

### 1.3 异步并发
Python 后端推荐使用 `FastAPI` + `uvicorn`，网络 I/O 密集型任务（如请求 OpenAI API、查询数据库）全部使用 `async/await`。

## 2. 部署方案

### 2.1 容器化
编写 `Dockerfile`，将应用、依赖库打包。
建议使用 Multi-stage build 减小镜像体积。

```dockerfile
FROM python:3.10-slim as builder
# ... install dependencies ...

FROM python:3.10-slim
COPY --from=builder /usr/local/lib/python3.10/site-packages /usr/local/lib/python3.10/site-packages
COPY . /app
WORKDIR /app
CMD ["uvicorn", "main:app", "--host", "0.0.0.0"]
```

### 2.2 模型服务化 (vLLM)
如果是私有化部署 LLM，强烈推荐使用 [vLLM](https://github.com/vllm-project/vllm)。
它采用了 PagedAttention 技术，能显著提高显存利用率和吞吐量（Throughput）。
