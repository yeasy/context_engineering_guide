## 14.5 性能优化与部署

系统跑通只是第一步，能在生产环境稳定运行才是关键。

### 14.5.1 性能优化

#### 语义缓存
对于相似的高频提问，直接返回缓存的答案，无需经过 RAG 流程。
*   使用 `GPTCache` 或 Redis。
*   **原理**：计算 query embedding，若与缓存中问题的相似度高于阈值，则直接返回（阈值需通过线上数据与误命中风险评估确定）。

#### 流式输出
LLM 生成速度较慢，必须使用 Server-Sent Events (SSE) 或 WebSocket 实现流式响应，让用户立刻看到首字，降低心理延迟。

#### 异步并发
Python 后端可使用 `FastAPI` + `uvicorn` 等异步框架，网络 I/O 密集型任务（如请求模型 API、查询数据库）尽量使用 `async/await`。

### 14.5.2 部署方案

#### 容器化
编写 `Dockerfile`，将应用、依赖库打包。
建议使用 Multi-stage build 减小镜像体积。

```text
FROM python:3.10-slim as builder
# ... install dependencies ...

FROM python:3.10-slim
COPY --from=builder /usr/local/lib/python3.10/site-packages /usr/local/lib/python3.10/site-packages
COPY . /app
WORKDIR /app
CMD ["uvicorn", "main:app", "--host", "0.0.0.0"]
```

#### 模型推理与基础设施优化

如果是私有化部署核心的语言模型引擎，系统级的底层优化对于降低延迟、提高吞吐（Goodput）并控制服务器成本（TCO）至关重要：

*   **PagedAttention 与显存池化**：传统的静态连续显存分配会导致严重的显存碎片化。现代推理引擎（以 vLLM 首创）引入了类似于操作系统虚拟内存分页的 PagedAttention 机制，将大模型的庞大 KV Cache 切分成固定大小的物理块。此举将显存浪费率从约 80% 压降至极低水平，使得单卡可以同时承载数倍于以往的并发请求（Batch Size）。
*   **全局状态管理与 Prompt Caching**：对于高频复用的系统设定、知识库模板或是智能体的系统级工具定义，系统不需要每次耗费算力从头消化。前沿框架（如 SGLang 的 Radix Attention）在显存中维护一棵全局并发的基数树（Radix Tree），只要匹配到历史前缀即可直接复用 KV Cache 物理块，大幅规避并跳过了最漫长的首代计算（Prefill）。
*   **分离式架构 (Disaggregated Serving)**：因为首字长计算（计算密集型）和自回归逐字生成（访存密集型）在物理上的诉求天然互斥。对于大规模的生产集群，常将这两个阶段彻底解耦：独立的“Prefill 集群”负责计算输入，“Decode 集群”则专职吐字，借助高速微秒级 RDMA 网络实现状态在两者之间的零拷贝传输。这种极致分离是万亿参数模型体系的当红架构范式。
*   **推理引擎选型**：不同的框架在取向上都有极大差异，例如注重开箱即用及广泛生态兼容的 vLLM、代表 NVIDIA 算力天花板的 TensorRT-LLM、或是针对多轮复杂推理且极大优化状态机并行的 SGLang 等，建议结合具体的并发负载特征与 GPU 硬件资源进行压测选型。
