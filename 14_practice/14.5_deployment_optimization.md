## 14.5 性能优化与部署

系统跑通只是第一步，能在生产环境稳定运行才是关键。

### 14.5.1 性能优化

#### 语义缓存

对于相似的高频提问，直接返回缓存的答案，无需经过 RAG 流程。
*   使用 `GPTCache` 或 Redis。
*   **原理**：计算 query embedding，若与缓存中问题的相似度高于阈值，则直接返回（阈值需通过线上数据与误命中风险评估确定）。

#### 流式输出

LLM 生成速度较慢，必须使用 Server-Sent Events (SSE) 或 WebSocket 实现流式响应，让用户立刻看到首字，降低心理延迟。

#### 异步并发

Python 后端可使用 `FastAPI` + `uvicorn` 等异步框架，网络 I/O 密集型任务（如请求模型 API、查询数据库）尽量使用 `async/await`。

### 14.5.2 部署方案

#### 容器化

编写 `Dockerfile`，将应用、依赖库打包。
建议使用 Multi-stage build 减小镜像体积。

```text
FROM python:3.10-slim as builder
# ... install dependencies ...

FROM python:3.10-slim
COPY --from=builder /usr/local/lib/python3.10/site-packages /usr/local/lib/python3.10/site-packages
COPY . /app
WORKDIR /app
CMD ["uvicorn", "main:app", "--host", "0.0.0.0"]
```

#### 模型推理与基础设施优化

如果是私有化部署核心的语言模型引擎，系统级的底层优化对于降低延迟、提高吞吐（Goodput）并控制服务器成本（TCO）至关重要：

*   **PagedAttention 与显存池化**：传统的静态连续显存分配会导致显存碎片化。现代推理引擎（以 vLLM 首创）引入了类似于操作系统虚拟内存分页的 PagedAttention 机制，将 KV Cache 切分成固定大小的物理块。此举可将显存浪费率从约 80% 压降至极低水平，使得单卡可同时承载数倍于以往的并发请求。
*   **全局状态管理与 Prompt Caching**：对于高频复用的系统设定、知识库模板或智能体的系统级工具定义，系统不需要每次从头计算。前沿框架（如 SGLang 的 Radix Attention）在显存中维护一棵全局并发的基数树（Radix Tree），匹配到历史前缀即可直接复用 KV Cache 物理块，显著减少 Prefill 阶段的计算开销。
*   **分离式架构 (Disaggregated Serving)**：Prefill（计算密集型）和 Decode（访存密集型）在资源诉求上天然互斥。大规模生产集群常将两个阶段解耦：独立的 Prefill 集群负责计算输入，Decode 集群专职生成输出，借助高速 RDMA 网络实现 KV Cache 状态的跨节点传输。这是当前超大规模模型服务的主流部署范式。
*   **推理引擎选型**：不同的框架在取向上有所差异，例如注重开箱即用及广泛生态兼容的 vLLM、代表 NVIDIA 算力天花板的 TensorRT-LLM、或是针对多轮复杂推理且深度优化状态机并行的 SGLang 等，建议结合具体的并发负载特征与 GPU 硬件资源进行压测选型。

---

**下一节**: [14.6 持续优化与迭代](14.6_optimization.md)
