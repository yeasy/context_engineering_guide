## 14.5 性能优化与部署

系统跑通只是第一步，能在生产环境稳定运行才是关键。

### 14.5.1 性能优化

#### 语义缓存
对于相似的高频提问，直接返回缓存的答案，无需经过 RAG 流程。
*   使用 `GPTCache` 或 Redis。
*   **原理**：计算 query embedding，若与缓存中问题的相似度高于阈值，则直接返回（阈值需通过线上数据与误命中风险评估确定）。

#### 流式输出
LLM 生成速度较慢，必须使用 Server-Sent Events (SSE) 或 WebSocket 实现流式响应，让用户立刻看到首字，降低心理延迟。

#### 异步并发
Python 后端可使用 `FastAPI` + `uvicorn` 等异步框架，网络 I/O 密集型任务（如请求模型 API、查询数据库）尽量使用 `async/await`。

### 14.5.2 部署方案

#### 容器化
编写 `Dockerfile`，将应用、依赖库打包。
建议使用 Multi-stage build 减小镜像体积。

```text
FROM python:3.10-slim as builder
# ... install dependencies ...

FROM python:3.10-slim
COPY --from=builder /usr/local/lib/python3.10/site-packages /usr/local/lib/python3.10/site-packages
COPY . /app
WORKDIR /app
CMD ["uvicorn", "main:app", "--host", "0.0.0.0"]
```

#### 模型服务化
如果是私有化部署 LLM，可考虑使用 [vLLM](https://github.com/vllm-project/vllm) 等推理服务框架。
不同推理引擎在吞吐、延迟、显存占用与特性支持上各有取舍，建议结合硬件与负载做压测选型。
