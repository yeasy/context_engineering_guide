# 附录 C：参考文献 (References)

本文档列出了书中引用的学术论文和关键技术报告，供读者深入研读。

## 学术论文

### 检索与 Context 长度
- **Lost in the Middle**: Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., & Liang, P. (2023). [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172). arXiv preprint arXiv:2307.03172.
  - *解读：揭示了模型在处理长上下文时，倾向于关注开头和结尾的信息，而忽略中间部分。*

- **Attention Is All You Need**: Vaswani, A., et al. (2017). [Attention Is All You Need](https://arxiv.org/abs/1706.03762). NIPS.
  - *解读：Transformer 架构的奠基之作，理解 Self-Attention 机制的必读文献。*

### RAG 与检索增强
- **RAG**: Lewis, P., et al. (2020). [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401). NeurIPS.
  - *解读：RAG 概念的开山之作。*

- **REPLUG**: Shi, W., et al. (2023). [REPLUG: Retrieval-Augmented Black-Box Language Models](https://arxiv.org/abs/2301.12652).
  - *解读：探讨了如何将检索器视为黑盒模型的插件进行优化。*

## 技术报告与基准测试

- **LongBench**: Bai, Y., et al. (2023). [LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding](https://arxiv.org/abs/2308.14508).
  - *解读：专门针对长上下文能力的评测基准。*

- **RAGAS**: Es, S., et al. (2023). [RAGAS: Automated Evaluation of Retrieval Augmented Generation](https://arxiv.org/abs/2309.15217).
  - *解读：RAG 系统的自动化评估框架。*
